{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Introduction\n",
    "\n",
    "- PyTorch is a Python based scientific computation library providing two high-level features :\n",
    " <br><br>   - Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU). According to https://pytorch.org/docs/stable/torch.html, the torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.\n",
    "<br><br>    - Deep neural networks built on a type-based automatic differentiation system (PyTorch is an optimized tensor library for deep learning using GPUs and CPUs).\n",
    "<br><br>   \n",
    "- Major released dates - Facebook's AI Research lab (FAIR) \n",
    "    - PyTorch 1.0 - October 2, 2018\n",
    "    - PyTorch 1.10 - October 21, 2021\n",
    "<br><br>  \n",
    "- PyTorch : Python API (https://pytorch.org/docs/stable/index.html)\n",
    "     - torch\n",
    "     - torch.nn\n",
    "     - torch.nn.functional\n",
    "     - torch.Tensor\n",
    "     - Tensor Attributes\n",
    "     - Tensor Views\n",
    "     - torch.autograd\n",
    "     - torch.cuda\n",
    "     - torch.cuda.amp\n",
    "     - etc.\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0+cu102'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch tensors\n",
    "\n",
    "- A torch tensor is a multi-dimensional matrix (array) containing elements of a single data type. Most PyTorch functionalities or features are built on tensors and tensor functions.\n",
    "<br><br>\n",
    "- A tensor can be 0 (scalar), 1 (vector), 2 (matrix), ..., or n dimensional.\n",
    "<br><br>\n",
    "- A tensor can be of any of the following supported data types:\n",
    "    - torch.float32 or torch.float\n",
    "    - torch.float64 or torch.double\n",
    "    - torch.float16 or torch.half\n",
    "    - torch.complex32\n",
    "    - torch.complex64\n",
    "    - torch.complex128 or torch.cdouble\n",
    "    - torch.uint8\n",
    "    - torch.int8\n",
    "    - torch.int32 or torch.int\n",
    "    - torch.int64 or torch.long\n",
    "    - torch.bool\n",
    "    - torch.quint8\n",
    "    - etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([5]), torch.uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple example of creating 1D tensor\n",
    "a = torch.ones (5, dtype=torch.uint8)\n",
    "torch.is_tensor(a), a.shape, a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, torch.Size([3, 5]), torch.float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple example of creating 2D tensor\n",
    "b = torch.ones (3, 5)\n",
    "torch.is_tensor(b), b.shape, b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6720, -1.0822,  2.2452, -0.4180,  0.8905],\n",
       "        [-0.4218, -2.1663, -0.6457,  0.1963, -1.0655],\n",
       "        [ 0.2046,  1.6135, -0.2824, -0.8712,  0.4065]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate tensor using the standard normal distribution\n",
    "x = torch.randn(3,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6720, -1.0822,  2.2452, -0.4180,  0.8905],\n",
       "        [-0.4218, -2.1663, -0.6457,  0.1963, -1.0655],\n",
       "        [ 0.2046,  1.6135, -0.2824, -0.8712,  0.4065]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple tensor arithmetic operators\n",
    "2.0 * x\n",
    "x + b\n",
    "x / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([1., 1., 1.]),\n",
       " tensor([[2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.],\n",
       "         [2., 2., 2.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple demo of Broadcasting semantics - the following works\n",
    "y = torch.ones (5,3)\n",
    "z = torch.ones(3)\n",
    "y, z, y+z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]),\n",
       " tensor([1., 1., 1.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple demo of Broadcasting semantics\n",
    "y = torch.ones (3,5)\n",
    "z = torch.ones(3)\n",
    "y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2f95f60cb53b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Simple demo of Broadcasting semantics - the following doesn't work\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Simple demo of Broadcasting semantics - the following doesn't work\n",
    "y + z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple demo of Broadcasting semantics - a fix of the last cell\n",
    "y, z.unsqueeze(-1) # add one dimension to z after the last dim in z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple demo of Broadcasting semantics \n",
    "y + z.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 8, 8, 4, 2],\n",
       "        [0, 9, 3, 8, 7],\n",
       "        [2, 5, 8, 0, 5]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple tensor reduction functions\n",
    "x = torch.randint(10,(3,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27, 27, 20])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()        # full reduction, a tensor of dimension 0 is retured\n",
    "x.sum(dim = 1) # reduction along dimension 1 axis (i.e, column-wise reduction), \n",
    "               # a tensor of dimension 1 is retured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27, 27, 20])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same results as those of last cell\n",
    "torch.sum(x)   # full reduction, a tensor of dimension 0 is retured\n",
    "torch.sum(x,1) # reduction along dimension 1 axis (i.e, column-wise reduction), \n",
    "               # a tensor of dimension 1 is retured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A matrix row (or column) normalization problem\n",
    "<br>\n",
    "- Given a 2D tensor (i.e., matrix), normalize all the rows (or columns) in the matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2, 3],[4,5,6],[7,8, 9]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 15, 24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all row in x - step 1\n",
    "x1 = x.sum(dim=1)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]),\n",
       " tensor([[ 6],\n",
       "         [15],\n",
       "         [24]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all row in x - step 2\n",
    "x2 = x1.unsqueeze(1) # add one more dimension at 1-th dimension\n",
    "x2.shape, x, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.3333, 0.5000],\n",
       "        [0.2667, 0.3333, 0.4000],\n",
       "        [0.2917, 0.3333, 0.3750]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all row in x - step 3\n",
    "x / x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.3333, 0.5000],\n",
       "        [0.2667, 0.3333, 0.4000],\n",
       "        [0.2917, 0.3333, 0.3750]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all row in x - in one step\n",
    "x / x.sum(1).unsqueeze(-1)       # -1 means to add one dimension after the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0833, 0.1333, 0.1667],\n",
       "        [0.3333, 0.3333, 0.3333],\n",
       "        [0.5833, 0.5333, 0.5000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all columns in x\n",
    "x/x.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA SEMANTICS\n",
    "- https://pytorch.org/docs/stable/notes/cuda.html\n",
    "\n",
    "torch.cuda is used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device. The selected device can be changed with a torch.cuda.device context manager.\n",
    "\n",
    "However, once a tensor is allocated, you can do operations on it irrespective of the selected device, and the results will be always placed in on the same device as the tensor.\n",
    "\n",
    "Cross-GPU operations are not allowed by default, with the exception of copy_() and other methods with copy-like functionality such as to() and cuda(). Unless you enable peer-to-peer memory access, any attempts to launch ops on tensors spread across different devices will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda')     # Default CUDA device\n",
    "cuda0 = torch.device('cuda:0')\n",
    "cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2.], device=cuda0)\n",
    "# x.device is device(type='cuda', index=0)\n",
    "y = torch.tensor([1., 2.]).cuda()\n",
    "# y.device is device(type='cuda', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2.], device='cuda:0'), tensor([1., 2.], device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.device(0):\n",
    "    # allocates a tensor on GPU 0\n",
    "    a = torch.tensor([[1., 1.],[1.,1.]], device=cuda)\n",
    "\n",
    "    # transfers a tensor from CPU to GPU 0\n",
    "    b = torch.randn(2,2).cuda()\n",
    "\n",
    "    # You can also use ``Tensor.to`` to transfer a tensor:\n",
    "    b2 = torch.randn(2,2).to(device=cuda)\n",
    "    \n",
    "    # matrix multiplication\n",
    "    c = a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.]], device='cuda:0'),\n",
       " tensor([[-0.3435,  1.3492],\n",
       "         [ 0.0152, -0.9463]], device='cuda:0'),\n",
       " tensor([[-0.3283,  0.4029],\n",
       "         [-0.3283,  0.4029]], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
